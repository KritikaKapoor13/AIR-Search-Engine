{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8G41EZrAdVz"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9IrTmsPCnIi",
        "outputId": "b85ce0ae-b240-4d6c-e1ce-c6b3532dc1df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoFgyn3ZCvGj"
      },
      "source": [
        "stopWords = stopwords.words(\"english\")\n",
        "stopWords.append(\"i\\'ve\")\n",
        "stopWords.append(\"i\\'m\")\n",
        "stopWords.append(\"he\\'d\")\n",
        "stopWords.append(\"she\\'d\")\n",
        "stopWords.append(\"i\\'d\")\n",
        "stopWords.append(\"i\\'ll\")\n",
        "stopWords.append(\"we\\'ll\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDWQRxIpDCpP"
      },
      "source": [
        "def lemmatize_sentence(sentence):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word in sentence.split():\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
        "    return \" \".join(lemmatized_sentence)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InTEQfVpLXTE"
      },
      "source": [
        "def special_characters_removal(word):\n",
        "  special=list(string.punctuation)\n",
        "  special.append(' ')\n",
        "  special.remove('*')\n",
        "  x=[]\n",
        "  for i in word:\n",
        "    if i not in special:\n",
        "      x.append(i)\n",
        "    else:\n",
        "      x.append(' ')\n",
        "  return ''.join(x)  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpiB4_QsDHji"
      },
      "source": [
        "def preprocessing(snippet):\n",
        "  snippet=snippet.split()\n",
        "  res=[]\n",
        "  for i in range(len(snippet)):\n",
        "    if snippet[i] not in stopWords and len(snippet[i])>1:\n",
        "      res.extend(special_characters_removal(snippet[i]).split())\n",
        "  snippet=\" \".join(res)\n",
        "  snippet=lemmatize_sentence(snippet)\n",
        "  return snippet.lower()  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbb-OBVdIeiD"
      },
      "source": [
        "def inverted_index_construction(file,location,posting_list):\n",
        "  file_df=pd.read_csv(location+file)\n",
        "  snippets=list(file_df['Snippet'])\n",
        "  result=[]\n",
        "  for i in snippets:\n",
        "    result.append(preprocessing(i))\n",
        "  for i in range(len(result)):\n",
        "    tokens=result[i].split()\n",
        "    for k in range(len(tokens)):\n",
        "      j=tokens[k]\n",
        "      if j in posting_list:\n",
        "        if file in posting_list[j]:\n",
        "          posting_list[j][file]['frequency']+=1\n",
        "          if i in posting_list[j][file]['records']:\n",
        "            posting_list[j][file]['records'][i]['frequency']+=1\n",
        "            posting_list[j][file]['records'][i]['position'].append(k)\n",
        "          else:\n",
        "            posting_list[j][file]['records'][i]={'position':[k],'frequency':1}\n",
        "        else:\n",
        "          posting_list[j]['doc_frequency']+=1\n",
        "          posting_list[j][file]={'frequency':1,'records':{i:{'position':[k],'frequency':1}}}\n",
        "      else:\n",
        "        posting_list[j]={file:{'frequency':1,'records':{i:{'position':[k],'frequency':1}}},'doc_frequency':1}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_5l9PolAsLy",
        "outputId": "edc89083-78ec-441b-cf58-fc304be9294d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "posting_list={}\n",
        "files=os.listdir('drive/My Drive/archive/TelevisionNews')\n",
        "location='drive/My Drive/archive/TelevisionNews/'\n",
        "for i in range(len(files)):\n",
        "  try:\n",
        "    inverted_index_construction(files[i],location,posting_list)\n",
        "  except:\n",
        "    print(files[i])\n",
        "f=open('drive/My Drive/AIR/inverted_index1.json','w')\n",
        "f.write(json.dumps(posting_list))\n",
        "f.close()\n",
        "%time"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN.200910.csv\n",
            "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
            "Wall time: 9.54 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUGLGjFZsQvc"
      },
      "source": [
        "def permutem(word,permutem_index):\n",
        "  i=0\n",
        "  while(i<=len(word)):\n",
        "    x=word[i:]+\"$\"+word[:i]\n",
        "    permutem_index[x]=word\n",
        "    i+=1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXeqo4XvMS6q"
      },
      "source": [
        "permutem_index={}\n",
        "for i in list(posting_list.keys()):\n",
        "  permutem(i,permutem_index)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTY4nW0kU6qg"
      },
      "source": [
        "f=open('drive/My Drive/AIR/permuterm1.json','w')\n",
        "f.write(json.dumps(permutem_index))\n",
        "f.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3HgRRGxIsyt"
      },
      "source": [
        "files=os.listdir('drive/My Drive/archive/TelevisionNews')\n",
        "files.sort()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu2VC4rm7cee",
        "outputId": "e8a79be2-39f0-4ee6-b7c5-f08b1e3eafa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "record_list={}\n",
        "location='drive/My Drive/archive/TelevisionNews/'\n",
        "prev=0\n",
        "for i in files:\n",
        "  try:\n",
        "    df=pd.read_csv(location+i)\n",
        "    record_list[i]=[len(df),prev]\n",
        "    prev+=len(df)\n",
        "  except:\n",
        "    print(i)\n",
        "    pass\n",
        "f=open('drive/My Drive/AIR/record_list.json','w')\n",
        "f.write(json.dumps(record_list))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN.200910.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoSlRqmIcGsJ",
        "outputId": "ba90d836-b061-438a-f102-e5ea214c4e0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(posting_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38231"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXEhKRhjqCds"
      },
      "source": [
        "import pandas as pd\n",
        "final=[]\n",
        "location='drive/My Drive/archive/TelevisionNews/'\n",
        "for i in files:\n",
        "  try:\n",
        "    df=pd.read_csv(location+i)\n",
        "    records=df.values.tolist()\n",
        "    final.extend(list(map(lambda x:[records[x][-1],i,x],range(len(records)))))\n",
        "  except:\n",
        "    pass\n",
        "f=open('drive/My Drive/AIR/snippet_df.txt','w')\n",
        "f.write(str(final))\n",
        "f.close()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLaSq1NEJAgh",
        "outputId": "5f5ae939-8f68-4cad-aed9-0fad8f21f6be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "final[:5]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['beena part to do. the airline industry has not been a part of this move to reduce carbon and teal last year. -- and teal. they agreed on a deal to curb greenhouse gases sol',\n",
              "  'BBCNEWS.201701.csv',\n",
              "  0],\n",
              " [\"it's beaten it by about 0.1, 0.12 degrees celsius. which doesn't seem like a lot, but in terms of the yearly variations, it is actually huge. part of this rise was caused by an el nino event, a warm ocean current that disrupts the world's weather. but scientists say greenhouse gases\",\n",
              "  'BBCNEWS.201701.csv',\n",
              "  1],\n",
              " ['contact more than expected, how. your co nta ct le ns more than expected, how. your contact lens has come out of your eye, good catch!. what does i do in terms of policy, already there is a pushback on global warming. interesting to see this against the backdrop of president trump coming into the oval office, saying things like climate change is something',\n",
              "  'BBCNEWS.201701.csv',\n",
              "  2],\n",
              " ['where every time a marketplace is closed down, another appears to replace it? inafew in a few minutes we are going to be talking about global warming. it is',\n",
              "  'BBCNEWS.201701.csv',\n",
              "  3],\n",
              " ['applause climate change, a controversial issue which has seen him at odds with the incoming president, was next on the speech agenda. take the challenge climate change.',\n",
              "  'BBCNEWS.201701.csv',\n",
              "  4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ozy9NmdSLN2",
        "outputId": "81f9fed7-08aa-4af3-dd28-ae0bc7006b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "preprocessing(\"president trump OR climate change\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'president trump or climate change'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GC6H2XB9KSm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}