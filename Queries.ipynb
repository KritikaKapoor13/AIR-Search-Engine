{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Queries.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFboIGSXdfyk",
        "outputId": "5e23a729-82c9-40e4-91d9-560947bb2425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install autocorrect"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/71/eb8c1f83439dfe6cbe1edb03be1f1110b242503b61950e7c292dd557c23e/autocorrect-2.2.2.tar.gz (621kB)\n",
            "\r\u001b[K     |▌                               | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 20.4MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 11.0MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 153kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 276kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 317kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 358kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 430kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 471kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 512kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 552kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 593kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 6.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.2.2-cp36-none-any.whl size=621491 sha256=ddda37aba6aa1458dd65de71d520a1bcbe19243ffffbd66a9c548ed147d13f1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/0b/7d/98268d64c8697425f712c897265394486542141bbe4de319d6\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAHMXAiUv054"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "from autocorrect import Speller"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGOGj4LlCqVm",
        "outputId": "d7f4e08e-6bf4-45e8-fcfa-170436e947f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YduoVHwticEi"
      },
      "source": [
        "f=open('drive/My Drive/AIR/inverted_index1.json','r')\n",
        "inverted_index=json.loads(f.read())\n",
        "f=open('drive/My Drive/AIR/record_list.json','r')\n",
        "record_list=json.loads(f.read())\n",
        "f=open('drive/My Drive/AIR/permuterm1.json','r')\n",
        "permuterm_index=json.loads(f.read())\n",
        "f=open('drive/My Drive/AIR/snippet_df.txt','r')\n",
        "snippet_df=eval(f.read())\n",
        "doc_location='drive/My Drive/archive/TelevisionNews/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2z4C6f5jlvD",
        "outputId": "4253375c-9475-42d9-ef1b-aeb7ad35f1ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stopWords = stopwords.words(\"english\")\n",
        "stopWords.append(\"i\\'ve\")\n",
        "stopWords.append(\"i\\'m\")\n",
        "stopWords.append(\"he\\'d\")\n",
        "stopWords.append(\"she\\'d\")\n",
        "stopWords.append(\"i\\'d\")\n",
        "stopWords.append(\"i\\'ll\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word in sentence.split():\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "def special_characters_removal(word):\n",
        "  special=list(string.punctuation)\n",
        "  special.append(' ')\n",
        "  special.remove('*')\n",
        "  x=[]\n",
        "  for i in word:\n",
        "    if i not in special:\n",
        "      x.append(i)\n",
        "    else:\n",
        "      x.append(' ')\n",
        "  return ''.join(x) \n",
        "\n",
        "def preprocessing(snippet):\n",
        "  snippet=snippet.split()\n",
        "  res=[]\n",
        "  for i in range(len(snippet)):\n",
        "    if snippet[i] not in stopWords and len(snippet[i])>1:\n",
        "      res.extend(special_characters_removal(snippet[i]).split())\n",
        "  snippet=\" \".join(res)\n",
        "  snippet=lemmatize_sentence(snippet)\n",
        "  return snippet.lower()  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7ainrEvkU60"
      },
      "source": [
        "## **`Boolean Queries`**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY9cQLV9kRh0"
      },
      "source": [
        "def mergeand(left_operand,right_operand):\n",
        "  doc_list=set(left_operand.keys()).intersection(set(right_operand.keys()))\n",
        "  final_record_list={}\n",
        "  for k in doc_list:\n",
        "    x=left_operand[k].intersection(right_operand[k])\n",
        "    if x:\n",
        "      final_record_list[k]=x\n",
        "  return final_record_list\n",
        "\n",
        "def mergeor(left_operand,right_operand):\n",
        "  doc_list=set(left_operand.keys()).union(set(right_operand.keys()))\n",
        "  final_record_list={k:left_operand[k].union(right_operand[k] if k in right_operand else set()) if k in left_operand else right_operand[k] for k in doc_list}\n",
        "  return final_record_list\n",
        "\n",
        "def get_not_posting_list(operand):\n",
        "  files=set(record_list.keys())\n",
        "  post_list={k:set(map(str,range(record_list[k][0]))).difference(operand[k] if k in operand else {}) for k in files}\n",
        "  return post_list\n",
        "\n",
        "def show_records(final):\n",
        "  result=[]\n",
        "  #result.insert(0,'Document Name',pd.Series)\n",
        "  #result.insert(1,'Row Number',int)\n",
        "  for i in final:\n",
        "    doc_l=doc_location+i\n",
        "    df=pd.read_csv(doc_l)\n",
        "    for j in final[i]:\n",
        "      result.append(df.loc[int(j)])\n",
        "      #result['Document Name'].loc[len(result)-1]=i\n",
        "  return result\n",
        "\n",
        "def get_posting_list(term):\n",
        "  if term not in inverted_index:\n",
        "    return dict()\n",
        "  x=list(inverted_index[term].keys())\n",
        "  x.remove('doc_frequency')\n",
        "  post_list={k:set(inverted_index[term][k]['records'].keys()) for k in x}\n",
        "  return post_list\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDGsUDH29pFZ"
      },
      "source": [
        "def parsebooleanquery(query):\n",
        "  query=query.replace('(',' ( ')\n",
        "  query=query.replace(')',' ) ')\n",
        "  precedence={}\n",
        "  precedence['NOT']=3\n",
        "  precedence['OR']=2\n",
        "  precedence['AND']=1\n",
        "  precedence['(']=0\n",
        "  precedence[')']=0\n",
        "  operand_stack=[]\n",
        "  operator_stack=[]\n",
        "  postfix_stack=[]\n",
        "  j=0\n",
        "  l=query.split()\n",
        "  while(j<len(l)):\n",
        "    i=l[j]\n",
        "    if i==\"(\":\n",
        "      operator_stack.append(i)\n",
        "    elif i==\")\":\n",
        "      while( operator_stack and operator_stack[-1]!=\"(\" ):\n",
        "        postfix_stack.append(operator_stack.pop())\n",
        "      operator_stack.pop()\n",
        "    elif i in precedence:\n",
        "      while(operator_stack and precedence[operator_stack[-1]]>precedence[i]):\n",
        "        postfix_stack.append(operator_stack.pop())\n",
        "      operator_stack.append(i)\n",
        "    else:\n",
        "      x=i\n",
        "      j+=1\n",
        "      while(j<len(l) and l[j] not in precedence):\n",
        "        x=x+\" \"+l[j]\n",
        "        j+=1\n",
        "      if not operator_stack or operator_stack[-1]!=\"NOT\":\n",
        "        operand_stack.append(x)\n",
        "      postfix_stack.append(x)\n",
        "      j-=1\n",
        "    j+=1\n",
        "  while(operator_stack):\n",
        "    postfix_stack.append(operator_stack.pop())\n",
        "  #print(postfix_stack,operand_stack)\n",
        "  return postfix_stack,operand_stack\n",
        "\n",
        "def eval_query(postfix_stack):\n",
        "  result=[]\n",
        "  for i in postfix_stack:\n",
        "    if i==\"AND\":\n",
        "      left_operand=result.pop()\n",
        "      right_operand=result.pop()\n",
        "      result.append(mergeand(left_operand,right_operand))\n",
        "    elif i==\"OR\":\n",
        "      left_operand=result.pop()\n",
        "      right_operand=result.pop()\n",
        "      result.append(mergeor(left_operand,right_operand))\n",
        "    elif i==\"NOT\":\n",
        "      operand=result.pop()\n",
        "      result.append(get_not_posting_list(operand))\n",
        "    else:\n",
        "      result.append(parsephrasequery(i) if '*' not in i else eval_wildcard_query(i))\n",
        "  return result[0]\n",
        "\n",
        "def get_results(query):\n",
        "  p_stack,o_stack=parsebooleanquery(query)\n",
        "  return eval_query(p_stack),o_stack"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqdon_QJeH_q"
      },
      "source": [
        "# Phrase query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhFn1RAWixj4"
      },
      "source": [
        "def parsephrasequery(query):\n",
        "  query=preprocessing(spellcheck(query))\n",
        "  tokens=query.split()\n",
        "  intermediate_result=get_posting_list(tokens[0])\n",
        "  for i in range(1,len(tokens)):\n",
        "    intermediate_result=mergeand(intermediate_result,get_posting_list(tokens[i]))\n",
        "  final_record_list=mergephrase(tokens,intermediate_result) if len(tokens)>1 else intermediate_result\n",
        "  return final_record_list\n",
        "\n",
        "def get_positional_list(doc_name, record_no, term):\n",
        "  return inverted_index[term][doc_name]['records'][record_no]['position'] \n",
        "\n",
        "def mergephrase(tokens,intersect_list):\n",
        "  final_record_list={}\n",
        "  for doc in intersect_list:\n",
        "    records=intersect_list[doc]\n",
        "    for record in records:\n",
        "      temp_pos=[]\n",
        "      for i in range(len(tokens)):\n",
        "        temp_pos.append(set(map(lambda x:x-i,get_positional_list(doc, record, tokens[i]))))\n",
        "      if (set(temp_pos[0].intersection(*temp_pos))):\n",
        "        if (doc not in final_record_list):\n",
        "          final_record_list[doc]=set()\n",
        "        final_record_list[doc].add(record)\n",
        "  return final_record_list  "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJwLRUHlPe4T"
      },
      "source": [
        "# Wildcard Queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr6lzEgAPmBa"
      },
      "source": [
        "def splitquery(query):\n",
        "  query1=query.split(\"*\")\n",
        "  if query1[0]=='' and len(query1)==3 and query1[2]=='':\n",
        "    return [query]\n",
        "  elif len(query1)==2 and (query1[0]=='' or  query1[1]==''):\n",
        "    return [query]\n",
        "  else:\n",
        "    result=[]\n",
        "    if query1[0]=='':\n",
        "      result.append(\"*\"+query1[1]+\"*\")\n",
        "      i=2\n",
        "    else:\n",
        "      result.append(query1[0]+\"*\")\n",
        "      i=1\n",
        "    while(i<len(query1)-1):\n",
        "      result.append(\"*\"+query1[i]+\"*\")\n",
        "      i+=1\n",
        "    if query1[i]!='':\n",
        "      result.append(\"*\"+query1[i])\n",
        "    return result\n",
        "  \n",
        "def permuterm_form(query):\n",
        "  query=query.split('*')\n",
        "  if query[0]=='' and len(query)==3 and query[2]=='': #*X*\n",
        "    actual_query=query[1]\n",
        "  elif query[0]=='' and len(query)==2:  #*X\n",
        "    actual_query=query[1]+\"$\"\n",
        "  elif query[1]=='' and len(query)==2:  #X*\n",
        "    actual_query=\"$\"+query[0]\n",
        "  return actual_query\n",
        "\n",
        "def parsewildcardquery(input_query):\n",
        "  x=splitquery(input_query)\n",
        "  result=get_wildcard_result(permuterm_form(x[0]))\n",
        "  for query in x[1:]:\n",
        "    result=result.intersection(get_wildcard_result(permuterm_form(query)))\n",
        "  tokens=input_query.split('*')\n",
        "  result1=[]\n",
        "  #print(tokens,result)\n",
        "  for i in result:\n",
        "    y=i\n",
        "    flag=0\n",
        "    for j in tokens:\n",
        "      if j=='':\n",
        "        pass\n",
        "      else:\n",
        "        pos=y.find(j)\n",
        "        if pos==-1:\n",
        "          flag=1\n",
        "          break\n",
        "        else:\n",
        "          y=y[pos+len(j):]\n",
        "    if flag==0:\n",
        "      result1.append(i)\n",
        "  return result1\n",
        "\n",
        "def get_wildcard_result(query):\n",
        "  result=[]\n",
        "  for i in permuterm_index:\n",
        "    if i.startswith(query):\n",
        "      result.append(permuterm_index[i])\n",
        "  return set(result)\n",
        "\n",
        "def eval_wildcard_query(query):\n",
        "  x=parsewildcardquery(query)\n",
        "  #print(x)\n",
        "  result=get_posting_list(x[0])\n",
        "  for i in x[1:]:\n",
        "    result=mergeor(get_posting_list(i),result)\n",
        "  return result"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW8bQaUgNCLt"
      },
      "source": [
        "# Ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukNO6WWRQcup"
      },
      "source": [
        "preprocessed_snippet_df=list(map(lambda x:preprocessing(x[0]),snippet_df))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6h69gzmX-ui"
      },
      "source": [
        "def ranked_results(query,k):\n",
        "  f=open('text2.txt','w')\n",
        "  t1=time.time()\n",
        "  results,tokens=get_results(query)\n",
        "  if not results:\n",
        "    return \"No results obtained.\"\n",
        "  tokens1=[]\n",
        "  for i in range(len(tokens)):\n",
        "    if '*' in tokens[i]:\n",
        "      tokens1.extend(list(parsewildcardquery(tokens[i])))\n",
        "    else:\n",
        "      tokens1.append(tokens[i])\n",
        "  #print(tokens1)\n",
        "  filtered_tfidf=[]\n",
        "  snippets=[]\n",
        "  for i in results:\n",
        "    for j in results[i]:\n",
        "      filtered_tfidf.append(record_list[i][1]+int(j))\n",
        "      snippets.append(preprocessed_snippet_df[filtered_tfidf[-1]])\n",
        "  results1=[]\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  X = vectorizer.fit_transform(snippets)\n",
        "  query_vec = vectorizer.transform([preprocessing(\" \".join(tokens1))])\n",
        "  #for i in range(len(filtered_tfidf)):\n",
        "  results1=cosine_similarity(X,query_vec).reshape((-1,))\n",
        "  results2=sorted(range(len(results1)),reverse=True, key=lambda x:results1[x])\n",
        "  t2=time.time()\n",
        "  print()\n",
        "  k=min(k,len(results2))\n",
        "  print(len(results2),\" results fetched in \",t2-t1,\"seconds. Top \",k,\"are being shown below - \")\n",
        "  print()\n",
        "  table=[]\n",
        "  for i in results2[:k]:\n",
        "      x=snippet_df[filtered_tfidf[i]]\n",
        "      table.append([x[1],x[2],x[0]])\n",
        "  print(tabulate(table,headers=[\"Document Name\",\"Row number\",\"Snippet\"]))\n",
        "  print()\n",
        "  for i in results2:\n",
        "      f.write(snippet_df[filtered_tfidf[i]][0])\n",
        "      f.write(\"\\n\")\n",
        "  f.close()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI4Rpe5IKcTx"
      },
      "source": [
        "# Spelling correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ASgsvsTdWV7"
      },
      "source": [
        "def spellcheck(query):\n",
        "  spell = Speller(lang='en')\n",
        "  query1=query\n",
        "  query = query.split()\n",
        "  corrected=[]\n",
        "  for i in range(len(query)):\n",
        "    if query[i] in inverted_index:\n",
        "      corrected.append(query[i])\n",
        "    else:\n",
        "      corrected.append(spell(query[i]))\n",
        "  corrected=\" \".join(corrected)\n",
        "  if corrected!=query1:\n",
        "    print(\"Did you mean\",corrected,\"?\")\n",
        "    #print(query,corrected)\n",
        "  return corrected"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RdVeANx4kyn"
      },
      "source": [
        "# Mixed Queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjh67xNrd4KB",
        "outputId": "d4c2fae7-60b5-497b-be70-70c58a5261d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ranked_results('war AND (iraq OR afghanistan)',10)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "70  results fetched in  0.2603425979614258 seconds. Top  10 are being shown below - \n",
            "\n",
            "Document Name         Row number  Snippet\n",
            "------------------  ------------  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "CNN.201102.csv                 0  wasn't talking enough about the war in iraq and afghanistan. kerry apologized to another woman who said he called her a neanderthal for not believing in global warming.\n",
            "MSNBC.201003.csv              95  them all overnight. well what do you make? it seems like there's so much before you now. the energy question in this country. our reliance on foreign oil. climate change, educational challenges. the war in iraq is winding down. the war in afghanistan continues.\n",
            "MSNBC.200910.csv             140  not achieving things that he set out to do, in terms of climate change and the war in iraq and afghanistan and other things of that nature. it comes in a week when the president declined to meet with the dailai lama.\n",
            "MSNBC.201003.csv              65  climate change, educational challenges. the war in iraq is winding down. the war in afghanistan continues. so many challenges for this president to get from here to the end of perhaps two terms. so many challenges. but the president has shown us\n",
            "MSNBC.201003.csv              88  climate change, educational challenges. the war in iraq is winding down. the war in afghanistan continues. so many challenges for this president to get from here to the end of perhaps two terms. so many challenges. but the president has shown us\n",
            "FOXNEWS.201409.csv           128  ending the wars in iraq and afghanistan. the president sees himself as a peacemaker, a civilizing force in a brutal world. he's much more interested in fighting global warming than some killers in a desert. but when the american people a\n",
            "FOXNEWS.200912.csv           839  8%, the wars in afghanistan and iraq. 7% cite reducing the deficit. addressing global warming is way down there at only 2 the side of the obama administration has pledged to support a plan for countries to kick in a $100\n",
            "FOXNEWS.201609.csv            68  he talked about ending the wars in iraq and afghanistan, fighting climate change and curving the proliferation of nuclear weapons. fast forward to today, varying degrees of success in some of those areas and very little\n",
            "FOXNEWS.201007.csv            73  frustration. during the exxon valdez, the economy was better. we weren't in wars in afghanistan and iraq. isolated up there and there weren't millions of people being affected. doctor, how do you see this? my thesis is that global warming is behind the anger and it's just bubbling to the surface,\n",
            "FOXNEWS.201007.csv            77  answers. right now, this is an example of frustration. during the exxon valdez, the economy was better. we weren't in wars in afghanistan and iraq. isolated up there and there weren't millions of people being affected. doctor, how do you see this? my thesis is that global warming is behind the anger and it's\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE6uiMoMLUug"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}